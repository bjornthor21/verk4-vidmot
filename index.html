<!-- <!DOCTYPE html>
<html>
<head>
    <title>Hand Detection</title>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
    <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/handpose"></script>
</head>
<body>
    <video id="video" width="640" height="480" autoplay></video>
    <script>
      async function main() {
        // Load the handpose model.
        const model = await handpose.load();

        // Start video stream
        const video = document.getElementById('video');
        navigator.mediaDevices.getUserMedia({ video: true }).then((stream) => {
            video.srcObject = stream;
        });

        video.onloadedmetadata = () => {
            detectHands(model, video);
        };
      }

      async function detectHands(model, video) {
        const predictions = await model.estimateHands(video);

        if (predictions.length > 0) {
            console.log(predictions);
            let procent = document.getElementById("procent");
            procent.textContent = predictions[0].handInViewConfidence;

            // Analyze the predictions to determine gestures
            const gesture = analyzeGestures(predictions[0].landmarks);
            console.log(gesture);  // Log the detected gesture
        }

        requestAnimationFrame(() => {
            detectHands(model, video);
        });
    }

      function analyzeGestures(landmarks) {
          const isIndexFingerUp = landmarks[8][1] < landmarks[6][1]; // y-coordinate comparison

          if (isIndexFingerUp) {
              return "Index Finger Up";
          }

          return "Unknown Gesture";
      }

      main();
    </script>
    <h1 id="procent"></h1>
</body>
</html> -->

<!-- <!DOCTYPE html>
<html>
<head>
    <title>MediaPipe Gesture Recognition</title>
</head>
<body>
  <script type="module" src="https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision/vision_bundle.js" crossorigin="anonymous"></script>
    <h1>MediaPipe Gesture Recognition Demo</h1>
    <video id="video" width="640" height="480" autoplay muted></video>
    <script>
      async function createGestureRecognizer() {
          // Access the webcam stream
          
          // Create task for gesture recognition
          const vision = await FilesetResolver.forVisionTasks(
            "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.4"
          );

          const gestureRecognizer = await GestureRecognizer.createFromOptions(vision, {
              baseOptions: {
                  modelAssetPath: "https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task",
                  delegate: "GPU",
              },
              runningMode: "VIDEO",
          });

          const videoElement = document.getElementById('video');
          const stream = await navigator.mediaDevices.getUserMedia({ video: true });
          videoElement.srcObject = stream;


          // Process the video stream
          videoElement.addEventListener('loadeddata', async () => {
              while (true) {
                  const results = await gestureRecognizer.recognizeForVideo(videoElement);
                  if (results) {
                      // Handle gesture recognition results here
                      console.log(results);
                  }
                  await new Promise(requestAnimationFrame);
              }
          });
      }

      createGestureRecognizer();
    </script>
</body>
</html> -->


<html lang='en'>

<head>
    <meta charset='UTF-8'>
    <title>Mediapipe Hand Gesture Recognizer</title>
</head>

<body>
    <div>
        <button id='webcamButton'>ENABLE WEBCAM</button>
        <div style='position: relative;'>
            <video id='webcam' autoplay></video>
            <canvas id='output_canvas' width='480' height='360'
                style='position: absolute; left: 0px; top: 0px;'></canvas>
            <h1 id='gesture_output'>
            <h2>X <span id='x_output'></span></h2>
            <h2>Y <span id='y_output'></span></h2>
        </div>
    </div>

    <script type='module'>
        import { GestureRecognizer, FilesetResolver, DrawingUtils } from 'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3';
        let gestureRecognizer;
        let runningMode = 'IMAGE';
        let enableWebcamButton;
        let webcamRunning = false;
        const videoWidth = '480px';
        const videoHeight = '360px';

        const createGestureRecognizer = async () => {
            const vision = await FilesetResolver.forVisionTasks('https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm');
            gestureRecognizer = await GestureRecognizer.createFromOptions(vision, {
                baseOptions: {
                    modelAssetPath: 'https://storage.googleapis.com/mediapipe-models/gesture_recognizer/gesture_recognizer/float16/1/gesture_recognizer.task',
                    delegate: 'GPU'
                },
                runningMode: runningMode
            });
        };
        createGestureRecognizer();

        const video = document.getElementById('webcam');
        const canvasElement = document.getElementById('output_canvas');
        const canvasCtx = canvasElement.getContext('2d');
        const gestureOutput = document.getElementById('gesture_output');
        const xOutput = document.getElementById('x_output');
        const yOutput = document.getElementById('y_output');

        enableWebcamButton = document.getElementById('webcamButton');
        enableWebcamButton.addEventListener('click', enableCam);

        function enableCam(event) {
            if (!gestureRecognizer) {
                alert('Please wait for gestureRecognizer to load');
                return;
            }
            if (webcamRunning === true) {
                webcamRunning = false;
            }
            else {
                webcamRunning = true;
            }
            // getUsermedia parameters.
            const constraints = {
                video: true
            };
            // Activate the webcam stream.
            navigator.mediaDevices.getUserMedia(constraints).then(function (stream) {
                video.srcObject = stream;
                video.addEventListener('loadeddata', predictWebcam);
            });
        }
        let lastVideoTime = -1;
        let results = undefined;
        async function predictWebcam() {
            const webcamElement = document.getElementById('webcam');
            // Now let's start detecting the stream.
            if (runningMode === 'IMAGE') {
                runningMode = 'VIDEO';
                await gestureRecognizer.setOptions({ runningMode: 'VIDEO' });
            }

            let nowInMs = Date.now();

            if (video.currentTime !== lastVideoTime) {
                lastVideoTime = video.currentTime;
                results = gestureRecognizer.recognizeForVideo(video, nowInMs);
            }

            canvasCtx.save();
            canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
            const drawingUtils = new DrawingUtils(canvasCtx);
            canvasElement.style.height = videoHeight;
            webcamElement.style.height = videoHeight;
            canvasElement.style.width = videoWidth;
            webcamElement.style.width = videoWidth;

            if (results.landmarks) {
                for (const landmarks of results.landmarks) {
                    drawingUtils.drawConnectors(landmarks, GestureRecognizer.HAND_CONNECTIONS, {
                        color: '#00FF00',
                        lineWidth: 5
                    });
                    drawingUtils.drawLandmarks(landmarks, {
                        color: '#FF0000',
                        lineWidth: 2
                    });
                }
            }

            canvasCtx.restore();
            if (results.gestures.length > 0) {
                gestureOutput.style.display = 'block';
                gestureOutput.style.width = videoWidth;
                gestureOutput.innerText = results.gestures[0][0].categoryName;
                
                parseFloat(xOutput.innerText = results.landmarks[0][0].x.toFixed(2));
                parseFloat(yOutput.innerText = results.landmarks[0][0].y.toFixed(2));
                
                console.log(gestureOutput.innerText)
            }
            else {
                gestureOutput.style.display = 'none';
            }

            if (webcamRunning === true) {
                window.requestAnimationFrame(predictWebcam);
            }
        }
    </script>


</body>

</html>